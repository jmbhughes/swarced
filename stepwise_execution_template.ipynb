{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show how to run a batch of lightcurves (e.g. a campaign) through our entire detection program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Find Eclipsing Binaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is identify eclipsing binaries so that we can hopefully discover a planet around them. To do this we have a few steps:\n",
    "<ol>\n",
    "<li> Run box-least squares on all the lightcurves to determine what periodic signals are in data </li>\n",
    "<li> Create a subset of EB candidates based on some selection criteria (e.g. depth, period, and strength of periodic signal) </li>\n",
    "<li> Individually confirm each EB noting fundamental paramaters: period, phase width of the primary and secondary eclipses, the separation fo the primary and secondary eclipses, and the central time of an eclipse\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Running box-least squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded <a href = https://github.com/dfm/python-bls> python bindings </a> (Ruth Angus and Daniel Foreman-Mackey) for BLS written originally by Kovacs et al. in Fortran. I have written just a simple wrapper for it (run_bls.py) that allows us to use default parameters (set manually in the code) to look at a large set of lightcurves with multi-core methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to run:\n",
    "    1. generate a list of lightcurves with absolute paths in a text file\n",
    "    2. Make sure retrieve in data.py is suited for your use (i.e. it is\n",
    "    lightcurves from Foreman-Mackey formatting, not straight from STSCI)\n",
    "    3. Determine an initial_time that you want to include all lightcurve data\n",
    "    after from (e.g. in Campaign 2 the first several data points before 2456900\n",
    "    were always bad. If you give that time in JD anything before then is not\n",
    "    included in the BLS analysis\n",
    "    4. Decide where to save the files. \n",
    "    5. nbins is the number of windows in the BLS code, used 5 for now. \n",
    "    6. Run from the command line:\n",
    "        python run_bls.py /path/to/your/list/of/lcs #ofCAMPAIGN INITIAL_TIME\n",
    "        /where/to/save/the/output nbin\n",
    "\n",
    "        ex:\n",
    "        python run_bls.py /k2_data/all_c2_lcs 2 2456900 /k2_data/c2_eb/bls.pkl\n",
    "        5\n",
    "\n",
    "Note the output is a pickled list of tuples. Where each tuple is the\n",
    "abbreviated BLS response for that EPIC: (EPIC, (best_period, best_power, depth,\n",
    "q, in1, in2)) where \n",
    "best_period is the best-fit period in the same units as time,\n",
    "best_power is the power at best_period,\n",
    "depth is the depth of the transit at best_period,\n",
    "q is the fractional transit duration,\n",
    "in1 is the bin index at the start of transit, and\n",
    "in2 is the bin index at the end of transit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filter EBs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each campaign has thousands of lightcurves, we can't individually determine if they're all EBs. You can do something like the following to generate a list of EB candidates from the BLs results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle, os\n",
    "import numpy as np\n",
    "path = '/k2_data/'\n",
    "sys.path.append(path)\n",
    "import swarced as sw\n",
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bls= pickle.load(open(\"/path/to/output/from/bls/above\",'r'))\n",
    "#Example: c1_eb = pickle.load(open(\"/Volumes/k2_data/c1_eb/bls.pkl\",'r'))\n",
    "#The above list of tuples is now (epic, (best_period, best_power, depth, q, in1, in2)) where:\n",
    "#best_period is the best-fit period in the same units as time,best_power is the power at best_period,\n",
    "#depth is the depth of the transit at best_period,q is the fractional transit duration,\n",
    "#in1 is the bin index at the start of transit, and in2 is the bin index at the end of transit.\n",
    "def is_Valid(result):\n",
    "        #True if Depth is positive and period is less than 30\n",
    "        return all([(result[1][3] > 0),(result[1][1]<30)])\n",
    "ebcandidates = np.array([result for result in bls if is_Valid(result)])\n",
    "strengths = [r[1][2] for r in ebcandidates]\n",
    "ebcandidates = ebcandidates[strengths.argsort()] #sort by strength of detection\n",
    "pickle.dump(ebcandidates[-600:], open(\"file_path\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more interactive way including plotting and picking a strength cutoff instead of just picking a number of features see the notebook pick_eb_c1 or pick_eb_c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Confirm EBs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are hopefully many false positives in your list so that you can assure you don't have any false negatives. That means we need to go through as humans and look at each lightcurve. I've written a little tool to take care of this too. It's interactive_clip_limits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to run: \n",
    "    1. BLS must be previously. Ideally you have filtered the results according\n",
    "    to stepwise_execution_template.ipynb. The output of filtering is at\n",
    "    blsreportpath. \n",
    "    2. limitsreportpath is where you want to save the results of your\n",
    "    interactive clipping\n",
    "    3. Directory is wherever /k2_data/ has been mounted (e.g.\n",
    "    /Volumes/k2_data/, /k2_data/, /mnt/k2_data/, ~/k2/)\n",
    "    4. Run this by:\n",
    "        python interactive_clip_limits.py /path/to/bls /path/to/limits\n",
    "        /k2_data/\n",
    "        ex: python interactive_clip_limits.py /k2_data/c3_eb/ebcandidates.pkl\n",
    "        /k2_data/c3_eb/interactive_results.pkl /k2_data/\n",
    "\n",
    "Features:\n",
    "    The output of this is a pickled array of lists structured:\n",
    "    [EPIC ID, phase_width_of_primary, phase_width_of_secondary, period,\n",
    "    separation_between_primary_and_secondary, central_time_of_primary, mode]\n",
    "    Mode is which radio button you've selected for that file. \n",
    "    The other nice feature of this is that you can exit while doing your\n",
    "    clipping and take a break. When you return (assuming you give it the exact\n",
    "    same commmand to run) your program will pick back up where you were. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a list of final EBs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this done, we want to make a final list of EBs as an ascii file (e.g. /k2_data/c1_eb/final_eb_list.txt). Load your interactive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactive = pickle.load(open(\"/path/to/interactive/results.pkl\",'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactive = [r for r in interactive if r[-1] == 'EB' or r[-1] == 'EB but check']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RETURNNNNNNNNNNNNNNNNNNN!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mask the EB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two methods of masking: simple masking and moving median masking. I'll illustrate moving median masking because that's more what we've been doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clip_params = ascii.read(\"/k2_data/c2_eb/final_eb_list.txt\")\n",
    "ls = os.listdir(\"/k2_data/c2_injection_trial_five/lcs/\")\n",
    "ls = np.array([l for l in ls if \"_injected.fits\" in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ls</b> could also be a list of files read in. You just need filepaths. First we must generate the arguments we're going to clip by. I'm going to use multiprocessing because with a sizeable number of lightcurves this could be slower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = []\n",
    "for fn in ls:\n",
    "    if int(fn[4:13]) in clip_params['EPIC']:\n",
    "        windowsize = 5\n",
    "        outpath = '/k2_data/c2_injection_trial_five/median_clip/'\n",
    "        inpath = '/k2_data/c2_injection_trial_five/lcs/'\n",
    "        epicID, period, t0, pwidth, swidth, sep = clip_params[np.where(int(fn[4:13])==clip_params['EPIC'])[0][0]]\n",
    "        #epicID, pwidth, swidth, period, sep, t0, mode = int(epicID), float(pwidth), float(swidth), float(period), float(sep), float(t0), str(mode)\n",
    "        args.append((epicID, 2, period, t0, outpath, sw.C2INITIALTIME, windowsize, '/k2_data/',inpath+ fn, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function just calls my clip_by_median functino I've already written\n",
    "def single_clip(args):\n",
    "    sw.data.clip_by_median(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "results = pool.map(single_clip, args)\n",
    "pool.close()\n",
    "pool.join()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ketu code by Foreman-Mackey needs a query file to give the parameters. In this step, we're also doing some checks to make sure that our lightcurve is even reasonable. Once again, this is for the more complicated injected case. You can get rid of a lot of checks if you're not worried about injections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"/k2_data/c2_injection_trial_five/median_clip/\" #where the clipped lightcurves are\n",
    "q_list = [fn for fn in os.listdir(path) if \".fits\" in fn]\n",
    "ref = ascii.read(\"/k2_data/c2_eb/final_eb_list.txt\") #the final list of EBs with their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ktwo204822463-c02_lpd-lc_040_injected_clipmed.fits\n"
     ]
    }
   ],
   "source": [
    "for lc in q_list:\n",
    "    try:\n",
    "        epic = int(lc[4:13])\n",
    "        if epic in ref['EPIC']:\n",
    "            loc = np.where(ref['EPIC']==epic)[0][0]\n",
    "            if fits.open(path + lc)[3].header['period'] < 70: #the injected planet has a period less than 70, delete for run w/o injections\n",
    "                ebperiod = ref['period'][loc]\n",
    "                if ebperiod * np.sqrt(8) < 70: #A planet is possible\n",
    "                    query = sw.query.get_planet_default(\"/mnt\" + path + lc,\"2\",\"/mnt/k2_data/\", ebperiod)\n",
    "                    if query != False:\n",
    "                        sw.query.save(query, path + lc.split(\".fits\")[0] + \".query\")\n",
    "    except:\n",
    "        print lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run ketu on clipped EBs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've finally gotten to the point where we can run ketu on the lightcurves! Yay! We have two main ways to do this: \n",
    "<ol>\n",
    "<li><b>multicore on a single machine</b> use multirun_ketu.py and see the top it for description</li>\n",
    "<li><b>multicore on multiple machines</b> use scoop_ketu.py and see the top of it for run description</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Recover ketu results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Marcus is silly, he hasn't yet tested and implemented the instantaneous saving method. (He got confused by some file io problems where it wasn't writing in a synchonous fashion. He understands now. Bug him to fix this!) So, we have to format our own table of results afterwards :(. Just run the below code making sure to change the paths accordingly. He's terribly sorry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/k2_data/c2_injection_trial_five/median_clip/\"\n",
    "contents = os.listdir(path)\n",
    "contents = np.array([fn for fn in contents if (\"_injected_clipmed.result\" in fn)])\n",
    "epicid = np.array([fn.split(\"-\")[0][4:] for fn in contents],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "inj_center, inj_period, inj_rpbyrs, inj_tdepth, inj_tctime, inj_prad, inj_srad, inj_smass, inj_impact = [],[], [], [], [], [], [], [], []\n",
    "inj_ttimes=[]\n",
    "lc_remaining = []\n",
    "fnlist = []\n",
    "for fn in contents:\n",
    "    if fn not in fails:\n",
    "        hdulist = fits.open(path + fn.split(\".\")[0] + \".fits\")\n",
    "        head = hdulist[3].header\n",
    "        inj_period += [head['PERIOD']]\n",
    "        inj_rpbyrs += [head['RRATIO']]\n",
    "        inj_tdepth += [head['TDEPTH']]\n",
    "        inj_tctime += [head['TCTIME']+hdulist[1].header['BJDREFI']]\n",
    "        inj_prad += [head['PRADRJ']]\n",
    "        inj_srad += [head['SRADRS']]\n",
    "        inj_smass += [head['MSTAR']]\n",
    "        inj_impact += [head['IMPACT']]\n",
    "        inj_center += [hdulist[3].data['center'][0] +hdulist[1].header['BJDREFI']]\n",
    "        fnlist.append(fn)\n",
    "        hdulist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epics = []\n",
    "for fn in contents:\n",
    "    if fn not in fails:\n",
    "        epics += [int(fn[4:13])]\n",
    "epics = np.array(epics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inj_period, inj_rpbyrs, inj_tdepth=np.array(inj_period), np.array(inj_rpbyrs), np.array(inj_tdepth)                                                   \n",
    "inj_tctime, inj_prad, inj_srad = np.array(inj_tctime), np.array(inj_prad), np.array(inj_srad)\n",
    "inj_smass, inj_impact =np.array(inj_smass), np.array(inj_impact)\n",
    "inj_center = np.array(inj_center)\n",
    "contents, epicid = np.array(contents), np.array(epicid)\n",
    "inj_ttimes = np.array(inj_ttimes)\n",
    "lc_remaining= np.array(lc_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recovered_period = []\n",
    "recovered_s2n = []\n",
    "second_period = []\n",
    "numpeaks = []\n",
    "recovered_depthivar, recovered_t0, recovered_phicnorm, recovered_rms, recovered_depth, recovered_depths2n = [],[],[],[],[],[]\n",
    "for fn in contents:\n",
    "    with open(path + fn,'r')as f:\n",
    "        if fn not in fails:\n",
    "            recovered_period_s, recovered_t0_s, recovered_depth_s, recovered_depths2n_s = [], [], [], []\n",
    "            result = pickle.load(f)\n",
    "            for i in range(25):\n",
    "                if i < len(result['peaks']):\n",
    "                    recovered_period_s += [result['peaks'][i]['period']]\n",
    "                    recovered_t0_s += [result['peaks'][i]['t0']]\n",
    "                    recovered_depth_s += [result['peaks'][i]['depth']]\n",
    "                    recovered_depths2n_s += [result['peaks'][i]['depth_s2n']]\n",
    "                else:\n",
    "                    recovered_period_s += [-1]\n",
    "                    recovered_t0_s += [-1]\n",
    "                    recovered_depth_s += [-1]\n",
    "                    recovered_depths2n_s += [-1]\n",
    "            numpeaks += [len(result['peaks'])]\n",
    "            recovered_period.append(recovered_period_s)\n",
    "            recovered_t0.append(recovered_t0_s)\n",
    "            recovered_depth.append(recovered_depth_s)\n",
    "            recovered_depths2n.append(recovered_depths2n_s)\n",
    "recovered_period = np.array(recovered_period)\n",
    "numpeaks = np.array(numpeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(recovered_period)):\n",
    "    ip = inj_period[i]\n",
    "    s = False\n",
    "    for j in range(len(recovered_period[i])):\n",
    "        if abs(inj_period[i] - recovered_period[i][j]) < 0.1:\n",
    "            s = True\n",
    "    success.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref = ascii.read(\"/k2_data/c2_eb/final_eb_list.txt\")\n",
    "eb_period = [ref['period'][np.argmax(ref['EPIC']==epic)] for epic in epics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"/k2_data/c2_injection_trial_five/med_results.txt\",'w') #save somewhere\n",
    "f.write(\"EPIC filename success eb_period inj_period inj_period_by_eb_period inj_radius_ratio inj_transit_depth inj_planet_radius inj_star_radius inj_star_mass inj_impact_param inj_center\\n\")\n",
    "for i in range(len(epics)):\n",
    "    #print i\n",
    "    l = \"{0:d} {1:s} {2:d} {3:f} {4:f} {12:f} {5:f} {6:f} {7:f} {8:f} {9:f} {10:f} {11:f}\\n\".format(epics[i], fnlist[i], success[i], \n",
    "                                                                                       eb_period[i], inj_period[i], inj_rpbyrs[i], inj_tdepth[i],\n",
    "                                                                                      inj_prad[i], inj_srad[i], inj_smass[i], inj_impact[i], \n",
    "                                                                                                    inj_center[i], inj_period[i]/eb_period[i])\n",
    "    \n",
    "    f.write(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Discovering the Planets! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where you are looking at things other than injected planets, you obviously have to individually examine the lightcurves for planet-like signatures. There is a notebook in nbs called validate_planets_c1.ipynb that shows an example. (Any of the validate notebooks will do the trick!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
